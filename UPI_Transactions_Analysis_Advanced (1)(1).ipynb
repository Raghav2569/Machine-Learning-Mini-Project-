{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05462f82",
   "metadata": {},
   "source": [
    "# Advanced UPI Transactions Analysis & Forecasting\n",
    "\n",
    "Comprehensive ML & DL pipeline on UPI transactions: EDA, feature engineering, classical ML (Linear Regression, Random Forest, XGBoost), and an optional LSTM time-series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup (Colab-ready)\n",
    "!pip install -q xgboost joblib\n",
    "\n",
    "import os, warnings, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "# For LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('Setup complete. Versions: pandas', pd.__version__, 'xgboost', xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fca671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load dataset (Upload or Drive)\n",
    "from google.colab import files\n",
    "print('Please upload the CSV file (e.g., upi-transactions-p2p-and-p2m.csv)')\n",
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "    data_path = fn\n",
    "print('Loaded:', data_path)\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cea26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Data overview\n",
    "print('Shape:', df.shape)\n",
    "print('\\nColumns:\\n', df.columns)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "display(df.info())\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Preprocessing: parse month/date and clean numeric columns\n",
    "# Assume there is a 'month' column as you confirmed\n",
    "date_col = 'month'\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "df = df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "possible_val_cols = [c for c in df.columns if 'val' in c.lower() or 'value' in c.lower() or 'amt' in c.lower() or 'amount' in c.lower() or 'total' in c.lower()]\n",
    "for c in possible_val_cols:\n",
    "    df[c] = pd.to_numeric(df[c].astype(str).str.replace(',','').str.replace('₹','').str.replace('Rs','').str.replace('rs',''), errors='coerce')\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print('Date column used:', date_col)\n",
    "print('Potential value cols:', possible_val_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — EDA & Visualizations\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "total_cols = [c for c in df.columns if 'total' in c.lower() or 'value' in c.lower() or 'val' in c.lower()]\n",
    "if len(total_cols)==0:\n",
    "    total_col = df.select_dtypes(include=[np.number]).columns[0]\n",
    "else:\n",
    "    total_col = total_cols[0]\n",
    "\n",
    "print('Using', total_col, 'for trend analysis')\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(df[date_col], df[total_col], marker='o')\n",
    "plt.title(f'{total_col} over time')\n",
    "plt.xlabel('Date'); plt.ylabel(total_col); plt.grid(True); plt.show()\n",
    "\n",
    "p2p_cols = [c for c in df.columns if 'p2p' in c.lower()]\n",
    "p2m_cols = [c for c in df.columns if 'p2m' in c.lower()]\n",
    "if p2p_cols and p2m_cols:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df[date_col], df[p2p_cols[0]], label='p2p')\n",
    "    plt.plot(df[date_col], df[p2m_cols[0]], label='p2m')\n",
    "    plt.legend(); plt.title('P2P vs P2M'); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.histplot(df[total_col].dropna(), kde=True)\n",
    "plt.title(f'Distribution of {total_col}'); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "sns.heatmap(num_df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation heatmap'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5769f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Feature Engineering\n",
    "data = df.copy()\n",
    "data['month_idx'] = np.arange(len(data))\n",
    "data['month_num'] = data[date_col].dt.month\n",
    "data['year'] = data[date_col].dt.year\n",
    "data['month_sin'] = np.sin(2 * np.pi * data['month_num']/12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['month_num']/12)\n",
    "\n",
    "p2p_cols = [c for c in df.columns if 'p2p' in c.lower()]\n",
    "p2m_cols = [c for c in df.columns if 'p2m' in c.lower()]\n",
    "if p2p_cols and p2m_cols:\n",
    "    data['p2p_to_p2m'] = data[p2p_cols[0]] / (data[p2m_cols[0]] + 1e-6)\n",
    "\n",
    "total_cols = [c for c in data.columns if 'total' in c.lower() or 'value' in c.lower() or 'val' in c.lower()]\n",
    "total_col = total_cols[0]\n",
    "data['total_val_lag1'] = data[total_col].shift(1)\n",
    "data['total_val_lag2'] = data[total_col].shift(2)\n",
    "data['total_val_rolling3'] = data[total_col].rolling(window=3).mean()\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210aede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Prepare features and target\n",
    "FEATURES = ['month_idx','month_sin','month_cos','total_val_lag1','total_val_lag2','total_val_rolling3']\n",
    "if 'p2p_to_p2m' in data.columns:\n",
    "    FEATURES.append('p2p_to_p2m')\n",
    "\n",
    "for f in FEATURES:\n",
    "    if f not in data.columns:\n",
    "        raise ValueError(f'Missing feature {f} in engineered data. Check preprocessing.')\n",
    "\n",
    "X = data[FEATURES].values\n",
    "y = data[total_col].values\n",
    "\n",
    "split_idx = int(len(data) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print('Train size:', X_train_s.shape, 'Test size:', X_test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_s, y_train)\n",
    "y_pred_lr = lr.predict(X_test_s)\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "print('Linear Regression:', metrics(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "rf.fit(X_train_s, y_train)\n",
    "y_pred_rf = rf.predict(X_test_s)\n",
    "print('Random Forest:', metrics(y_test, y_pred_rf))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "for f, imp in sorted(zip(FEATURES, importances), key=lambda x: x[1], reverse=True):\n",
    "    print(f, '->', round(imp,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — XGBoost\n",
    "xg_reg = xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, random_state=42)\n",
    "xg_reg.fit(X_train_s, y_train, eval_metric='rmse', verbose=False)\n",
    "y_pred_xgb = xg_reg.predict(X_test_s)\n",
    "print('XGBoost:', metrics(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 — Actual vs Predicted Plot\n",
    "test_dates = data[date_col].iloc[split_idx:].dt.strftime('%Y-%m').tolist()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_dates, y_test, marker='o', label='Actual')\n",
    "plt.plot(test_dates, y_pred_lr, marker='o', label='LinearReg')\n",
    "plt.plot(test_dates, y_pred_rf, marker='o', label='RandomForest')\n",
    "plt.plot(test_dates, y_pred_xgb, marker='o', label='XGBoost')\n",
    "plt.xticks(rotation=45); plt.legend(); plt.title('Actual vs Predicted total_val'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 — Save models & scaler\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "joblib.dump(scaler, 'artifacts/scaler.pkl')\n",
    "joblib.dump(lr, 'artifacts/linear_model.pkl')\n",
    "joblib.dump(rf, 'artifacts/rf_model.pkl')\n",
    "joblib.dump(xg_reg, 'artifacts/xgb_model.pkl')\n",
    "print('Saved artifacts to ./artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b713a1",
   "metadata": {},
   "source": [
    "## LSTM (sequence) model - Optional\n",
    "Small LSTM model to demonstrate sequence forecasting. With limited data LSTM may overfit; this is for demonstration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 — Prepare sequences for LSTM\n",
    "series = data[total_col].values\n",
    "mms = MinMaxScaler()\n",
    "series_scaled = mms.fit_transform(series.reshape(-1,1)).flatten()\n",
    "\n",
    "SEQ_LEN = 6\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQ_LEN, len(series_scaled)):\n",
    "    X_seq.append(series_scaled[i-SEQ_LEN:i])\n",
    "    y_seq.append(series_scaled[i])\n",
    "X_seq = np.array(X_seq); y_seq = np.array(y_seq)\n",
    "\n",
    "split_seq = int(len(X_seq) * 0.8)\n",
    "X_seq_train, X_seq_test = X_seq[:split_seq], X_seq[split_seq:]\n",
    "y_seq_train, y_seq_test = y_seq[:split_seq], y_seq[split_seq:]\n",
    "print('LSTM sequences sizes:', X_seq_train.shape, X_seq_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Train small LSTM\n",
    "tf.random.set_seed(42)\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X_seq_train.shape[1], 1)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_seq_train.reshape(-1, SEQ_LEN, 1), y_seq_train, validation_split=0.1, epochs=200, batch_size=8, callbacks=[es], verbose=0)\n",
    "print('LSTM trained. Best val loss:', min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15 — Evaluate LSTM\n",
    "y_seq_pred = model.predict(X_seq_test.reshape(-1, SEQ_LEN, 1)).flatten()\n",
    "y_seq_test_inv = mms.inverse_transform(y_seq_test.reshape(-1,1)).flatten()\n",
    "y_seq_pred_inv = mms.inverse_transform(y_seq_pred.reshape(-1,1)).flatten()\n",
    "\n",
    "print('LSTM metrics:', metrics(y_seq_test_inv, y_seq_pred_inv))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(y_seq_test_inv)), y_seq_test_inv, label='Actual')\n",
    "plt.plot(range(len(y_seq_pred_inv)), y_seq_pred_inv, label='Predicted')\n",
    "plt.legend(); plt.title('LSTM Predictions vs Actual'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583404cd",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "- Summarize model comparison and suggest best model(s) for forecasting.\n",
    "- Limitations: small sample size, need for external features like holidays and economic indicators.\n",
    "- Next steps: hyperparameter tuning, time-series cross-validation, deploy best model as REST API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
